import { Callout } from "nextra-theme-docs";
import Navigation from "components/navigation";

# Adding Nodes to a Spheron Provider

This document provides step-by-step instructions for adding both CPU and GPU nodes to a Spheron cluster. It includes preparations, script modifications, and necessary commands to ensure successful integration and functionality of the nodes within the cluster.

1. [Adding a CPU Node](/spheron-protocol/provider-onboarding/spheron-add-node#adding-a-cpu-node)
2. [Adding a GPU Node](/spheron-protocol/provider-onboarding/spheron-add-node#adding-a-gpu-node)

## Adding a CPU Node

To add a CPU node to your Spheron cluster, follow these steps:

### Preparing a Node for Installation

#### 1. Prepare a node for the installation using Ansible from the [earlier step](/spheron-protocol/provider-onboarding/setup-ansible).
#### 2. Clone the Provider Deployment Repository

If you haven't already, clone the Spheron provider deployment repository: [spheronFdn/provider-deployment](https://github.com/spheronFdn/provider-deployment)

```
git clone https://github.com/spheronFdn/provider-deployment.git
```

Repo has the following file structure: 
![Provider Deployment Repo](assets/provider-deployment-repo.png)

#### 3. Edit the Inventory File

Open `playbook/inventory.ini` and update it with your server details. Example:

<Callout type="info">
Ensure you set up the server IP, username, and SSH key correctly.
</Callout>

```
[new-instance] ansible_host=[23.158.40.38] ansible_user=root ansible_ssh_private_key_file=[~/.ssh/id_rsa]
```
#### 4. Execute the Ansible Playbook

```
cd playbook
ansible-playbook -i inventory.ini playbook.yml
```
- The server will restart after the execution. When prompted, say `No` to the request for the first node.

![Provider Deployment Repo](assets/cpu-node-1.png)

- Follow the Prompt like this:

![Provider Deployment Repo](assets/cpu-node-2.png)

#### 5. SSH into the Master Node
SSH into the first node (master node) of the cluster and follow the steps:

- Add the Node Using the `add-agent.sh` Script, run the following commands:

```
sudo su spheron
wget -q https://raw.githubusercontent.com/spheronFdn/provider-deployment/devnet/scripts/add-agent.sh
```

- Use Vim or Nano to edit the `add-agent.sh` script. Update the master node IP and add child node IPs:

```sh
vim add-agent.sh
```
Edit the following lines:

```sh
SPHERON_NODE1_IP=[134.195.196.81] # your master node

# all you child nodes
nodes=(
    ["spheron-node2"]="134.195.196.213" ## add nodes like this in the list
)
```
- Run the Script on the Master Node

```sh
sudo chmod +x add-agent.sh
./add-agent.sh
```



## Adding a GPU Node

To add a GPU node to your Spheron cluster, follow these steps:

### Preparing a Node for Installation

#### 1. Prepare a node for the installation using Ansible from the [earlier step](/spheron-protocol/provider-onboarding/setup-ansible).
#### 2. Clone the Provider Deployment Repository

If you haven't already, clone the Spheron provider deployment repository: [spheronFdn/provider-deployment](https://github.com/spheronFdn/provider-deployment)

```
git clone https://github.com/spheronFdn/provider-deployment.git
```

Repo has the following file structure: 
![Provider Deployment Repo](assets/provider-deployment-repo.png)

#### 3. Edit the Inventory File

Open `playbook/inventory.ini` and update it with your server details. Example:

<Callout type="info">
Ensure you set up the server IP, username, and SSH key correctly.
</Callout>

```
[new-instance-gpu] ansible_host=[23.158.40.38] ansible_user=root ansible_ssh_private_key_file=[~/.ssh/id_rsa]
```
#### 4. Execute the Ansible Playbook

```
cd playbook
ansible-playbook -i inventory.ini playbook.yml
```
- The server will restart after the execution. When prompted, say `No` to the request for the first node.

![Provider Deployment Repo](assets/cpu-node-1.png)

- Follow the Prompt like this:

![Provider Deployment Repo](assets/cpu-node-2.png)

It will isntall the GPU drivers and some scripts.

#### 5. SSH into the Master Node
SSH into the first node (master node) of the cluster and follow the steps:

- Add the Node Using the `add-agent.sh` Script, run the following commands:

```
sudo su spheron
wget -q https://raw.githubusercontent.com/spheronFdn/provider-deployment/devnet/scripts/add-agent.sh
```

- Use Vim or Nano editor to edit the `add-agent.sh` script. Update the master node IP and add child node IPs:

```sh
vim add-agent.sh
```
- Edit the following lines:

```sh
SPHERON_NODE1_IP=134.195.196.81 # your master node

# all you child nodes
nodes=(
    ["spheron-node2"]="134.195.196.213" ## add nodes like this in the list
)
```
- Run the Script on the Master Node

```sh
sudo chmod +x add-agent.sh
./add-agent.sh
```

### Install the Nvidia Driver (First GPU Node Only)

1. If this is the first GPU node, run these commands:

<Callout type="info">
  **NOTE:** Only Execute if this is the first GPU Node.
</Callout>

```
sudo su spheron
helm repo add nvidia https://helm.ngc.nvidia.com/nvidia
helm repo add nvdp https://nvidia.github.io/k8s-device-plugin

helm repo update

kubectl apply -f /home/spheron/gpu-nvidia-runtime-class.yaml

helm upgrade -i nvdp nvdp/nvidia-device-plugin \
  --namespace nvidia-device-plugin \
  --create-namespace \
  --version 0.14.5 \
  --set runtimeClassName="nvidia"
```

2. Create the Nvidia RuntimeClass (First GPU Node Only) using the root user.

```
sudo su 
```
```
# Create NVIDIA RuntimeClass
cat > /home/spheron/gpu-nvidia-runtime-class.yaml <<EOF
kind: RuntimeClass
apiVersion: node.k8s.io/v1
metadata:
  name: nvidia
handler: nvidia
EOF
```

### Configure the New Node on Master node

1. SSH into the new node and check if the file `/etc/rancher/k3/config.yaml` exists:

```
cat /etc/rancher/k3/config.yaml
```
2. If the above command doesn't show any output, create the file using following command:

```
cat > /etc/rancher/k3/config.yaml <<'EOF'
containerd_additional_runtimes:
  - name: nvidia
    type: "io.containerd.runc.v2"
    engine: ""
    root: ""
    options:
      BinaryName: '/usr/bin/nvidia-container-runtime'
EOF
```

3. Now restart k3s using

```
sudo systemctl restart k3s
```

### Create a GPU Test Pod (First GPU Node Only)

SSH back to the master node and create a GPU test pod to check if the GPU is configured successfully with Kubernetes.

```
cat > gpu-test-pod.yaml << EOF
apiVersion: v1
kind: Pod
metadata:
  name: nbody-gpu-benchmark
  namespace: default
spec:
  restartPolicy: OnFailure
  runtimeClassName: nvidia
  containers:
  - name: cuda-container
    image: nvcr.io/nvidia/k8s/cuda-sample:nbody
    args: ["nbody", "-gpu", "-benchmark"]
    resources:
      limits:
        nvidia.com/gpu: 1
    env:
    - name: NVIDIA_VISIBLE_DEVICES
      value: all
    - name: NVIDIA_DRIVER_CAPABILITIES
      value: all
EOF
```
```
kubectl apply -f gpu-test-pod.yaml
echo "Waiting 60 seconds for the test pod to start..."
sleep 60
kubectl get pods -A -o wide
kubectl logs nbody-gpu-benchmark
kubectl delete pod nbody-gpu-benchmark
```

<Callout type="info">
  Do the next step on master node!!!
</Callout>

### Verify the node has GPU labels (On Master Node)

```
kubectl describe node <node-name> | grep -A10 Labels
```

<Callout type="info">
  Do the next step on new GPU node!!!
</Callout>

### Update Provider Capabilities (On New GPU Node)
On the new GPU node, update the provider capabilities:

```
sphnctl wallet use --key-secret testPassword --name wallet
sphnctl provider update --config /home/spheron/.spheron/config.json
```

### Restart the Provider (On New GPU Node)
Restart the provider on new GPU node to apply the new capabilities:

```
kubectl rollout restart statefulset/spheron-provider -n spheron-services  
```

Now your provider should have GPU working ðŸš€ðŸš€ðŸš€

<Navigation name="Adding Nodes to a Spheron Cluster" />
