import { Callout } from "nextra-theme-docs";
import Navigation from "components/navigation";

## Setup Provider

This guide provides detailed instructions on how to set up a Spheron provider. The process involves several steps, including SSH access, environment setup, ingress configuration, GPU cluster setup, provider installation, operator installation, and optional persistent storage setup.

Follow the steps below to complete the setup.

## Initialize setup

### 1. SSH into the Provider Instance

Use the following command to SSH into your provider instance:

```
ssh -i [ssh_key] root@[provider-ip]
```

### 2. Initialize Configuration Prompts

1. Upon the first setup, you will be prompted with a question. Respond with `y` if this is your first node setup for the provider.

![Provider Setup 1](assets/provider-setup-1.png)

2. Respond with `y` if you are setting up the node for GPU. 

<Callout type="info">
**Note:** This will be prompted only if you have a GPU in the server.
</Callout>

![Provider Setup 2](assets/provider-setup-2.png)

3. If you already have a wallet, provide the mnemonic. Otherwise, choose `n` to let the script create a new wallet for you.

![Provider Setup 3](assets/provider-setup-3.png)

4. Provide the domain you attached in the [playbook hostname](/spheron-protocol/provider-onboarding/prepare-linux-instances#step-3-edit-playbook).

![Provider Setup 4](assets/provider-setup-4.png)

5. After the successful script run, you would see a output like below.

![Provider Setup 5](assets/provider-setup-5.png)


## Deposit SPH to the providerâ€™s wallet

1. Use the following command to SSH into your provider instance

```
ssh -i [ssh_key] root@[provider-ip]
```

2. Change user to `spheron`

```
sudo su spheron
```
```
cd
```

3. Run the following command to get your wallet address. 

```
cat ~/.spheron/wallet.json
```
You will receive output similar to: 
```
{"address":"d5a824a1b4a7de20ef1fb697d13a7e4dc3147a28","crypto":{"ci...
```

4. Go to the [**Faucet**](https://spheron-devnet.hub.caldera.xyz/) to obtain some test gas tokens for the address you got from the previous step. These tokens will allow you to perform on-chain transactions.

<Callout type="info">
To retrieve the mnemonic, run the following command:

```
cat /home/spheron/logs/installer/wallet.log | grep mnemonic
```
</Callout>

## Registering a Provider

To register a provider, use the following command:

<Callout type="info">
- Update the region in the config to reflect where your provider is located. If you have servers in different locations, create multiple providers, each with its own region.
- Update the payment tokens in the config to include all the tokens you want to accept for payments.
- Lastly, update the hostname to match the domain you configured at the beginning of the setup, as this will be exposed to users.
</Callout>

```
spheron provider add --from ~/.spheron/wallet.json --key-secret testPassword --payment-token "0x24A05B480235Ccb450bf7Ce7e9F65072Ed732292" --region [us-central-devnet] --domain [provider.devnetcsphn.com] --attributes ["region=us-central-devnet"]
```

### Adding Multiple Payment Tokens

You can add multiple tokens using the `--payment-token` flag as shown below:

```
--payment-token "t1,t2"
```

### Provider Capabilities

Each provider has certain properties defned by the attribute flag for example: GPU Model, Region, and Storage options.

- Example of GPU capability: 

```
spheron.network/capabilities.gpu.vendor.nvidia.model.p4=true 
```

- Example of Storage capability: 

```
spheron.network/capabilities.storage.class.beta1=true
```

### Changing Provider Attributes

To change the provider capabilities, modify the `--attributes` field like this:

<Callout type="info">
**Note:** For detailed information on GPU support and the corresponding labels, please refer to the [GPU support page](/spheron-protocol/gpu-tiering). The GPU capability attributes should be selected from the GPU support list and must match the GPU model of all the nodes. Providers can specify multiple GPU capabilities as comma-separated values.
</Callout>

```
--attributes "region=us-east,capabilities/gpu/vendor/nvidia/model/p4=true" 
```
The full command would look like this:

```
spheron provider add --from ~/.spheron/wallet.json --key-secret testPassword --payment-token "0x24A05B480235Ccb450bf7Ce7e9F65072Ed732292" --region us-central-devnet --domain provider.devnetcsphn.com --attributes "region=us-east,capabilities/gpu/vendor/nvidia/model/p4=true"
```
<Callout type="info">
Note: You can modify and change provider details using provider `add command`.
</Callout>

## Setup Environment

Setup the environment by running command below.

```
cd /home/spheron

export KUBECONFIG=/home/spheron/.kube/kubeconfig

git clone https://github.com/spheronFdn/provider-helm-charts.git
cd provider-helm-charts/charts

helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo add rook-release https://charts.rook.io/release
helm repo add nvidia https://helm.ngc.nvidia.com/nvidia

helm repo update

kubectl create ns spheron-services
kubectl label ns spheron-services spheron.network/name=spheron-services spheron.network=true
kubectl create ns lease
kubectl label ns lease spheron.network=true
wget https://spheron-release.s3.amazonaws.com/crd/crd.yaml
kubectl apply -f crd.yaml
```

## Setup Ingress

Run the below commean to setting up Ingress.

```
cat > ingress-nginx-custom.yaml << EOF
controller:
  service:
    type: ClusterIP
  ingressClassResource:
    name: "spheron-ingress-class"
  kind: DaemonSet
  hostPort:
    enabled: true
  admissionWebhooks:
    port: 7443
  config:
    allow-snippet-annotations: false
    compute-full-forwarded-for: true
    proxy-buffer-size: "16k"
  metrics:
    enabled: true
  extraArgs:
    enable-ssl-passthrough: true
tcp:
  "8443": "spheron-services/spheron-provider:8443"
  "8444": "spheron-services/spheron-provider:8444"
  "1317": "spheron-services/spheron-node-1:1317"
  "9090":  "spheron-services/spheron-node-1:9090"
  "26656": "spheron-services/spheron-node-1:26656"
  "26657": "spheron-services/spheron-node-1:26657"
EOF
```

```
helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
    --namespace ingress-nginx --create-namespace \
    -f ingress-nginx-custom.yaml

kubectl label ns ingress-nginx app.kubernetes.io/name=ingress-nginx app.kubernetes.io/instance=ingress-nginx
kubectl label ingressclass spheron-ingress-class spheron.network=true
```

## Only For the GPU Cluster

1. Start with the below script to setup the helm chart for the GPU provider deployment.

```
helm repo add nvidia https://helm.ngc.nvidia.com/nvidia
helm repo add nvdp https://nvidia.github.io/k8s-device-plugin

helm repo update


# Create NVIDIA RuntimeClass
cat > /home/spheron/gpu-nvidia-runtime-class.yaml <<EOF
kind: RuntimeClass
apiVersion: node.k8s.io/v1
metadata:
  name: nvidia
handler: nvidia
EOF

kubectl apply -f /home/spheron/gpu-nvidia-runtime-class.yaml

helm upgrade -i nvdp nvdp/nvidia-device-plugin \
  --namespace nvidia-device-plugin \
  --create-namespace \
  --version 0.14.5 \
  --set runtimeClassName="nvidia"
```
The script we executed in the initial steps creates a configuration file at `/etc/rancher/k3/config.yaml` location. Verify if it is present or else create it. 

2. For creating the configuration file use the following command. Run the command one by one:

```
sudo su
```
```
cat > /etc/rancher/k3/config.yaml <<'EOF'
containerd_additional_runtimes:
  - name: nvidia
    type: "io.containerd.runc.v2"
    engine: ""
    root: ""
    options:
      BinaryName: '/usr/bin/nvidia-container-runtime'
EOF
```
```
sudo su spheron
```
3. Now create a GPU test pod to check if the GPU is configured successfully with Kubernetes. Run the command one by one:

```
cat > gpu-test-pod.yaml << EOF
apiVersion: v1
kind: Pod
metadata:
  name: nbody-gpu-benchmark
  namespace: default
spec:
  restartPolicy: OnFailure
  runtimeClassName: nvidia
  containers:
  - name: cuda-container
    image: nvcr.io/nvidia/k8s/cuda-sample:nbody
    args: ["nbody", "-gpu", "-benchmark"]
    resources:
      limits:
        nvidia.com/gpu: 1
    env:
    - name: NVIDIA_VISIBLE_DEVICES
      value: all
    - name: NVIDIA_DRIVER_CAPABILITIES
      value: all
EOF
```
```
kubectl apply -f gpu-test-pod.yaml
echo "Waiting 60 seconds for the test pod to start..."
sleep 60
kubectl get pods -A -o wide
kubectl logs nbody-gpu-benchmark
kubectl delete pod nbody-gpu-benchmark
```

We need to add some labels to the GPU nodes `spheron.network/capabilities.gpu.vendor.nvidia.model.gpumodel=true` to activate GPU.

<Callout type="info">
**Note:** For detailed information on GPU support and the corresponding labels, please refer to the [GPU support page]([GPU support](/spheron-protocol/gpu-tiering)). The capabilities should be selected from the table provided on that page.
</Callout>


4. Find the GPU model of the node using this command 

```
nvidia-smi --query-gpu=gpu_name --format=csv,noheader

The output will look like below

```
TESLA P4
```

5. Fetch the node name using following command:

```
kubectl get nodes
```

6. To apply the label use the following command.
<Callout type="info">
**Note:** Update the node name from the above command and gpu capability using the table provided in the [Supported GPU](/spheron-protocol/gpu-tiering)
</Callout>

```
kubectl label node [node-name] [gpu-capability]
```

## Add Pricing

The init scripts download both the pricing script and price config. Price config is available at `/home/spheron/price.json`. It is a JSON file with the following data.

```sh
{
  "cpu": "3.00",
  "memory": "1.00",
  "ephemeral": "0.03",
  "hdd": "0.02",
  "ssd": "0.04",
  "nvme": "0.05",
  "endpoint": "0.06",
  "ip": "6",
  "gpu": "p4=50,rtx4090=80"
}
```
<Callout type="info">
**Note:** 
1. That all the prices are per unit price in USD value. For example, the CPU is set to 3 USD per 1 CPU / Thread per Month and similarly, others are set like that. For GPU, if you have multiple of them, you need to pass it in comma separated and the value should be set to unit price of the 1 GPU for a month.
2. Update this file using Vim or Nano with the values you want.
</Callout>

```
vim /home/spheron/price.json
```
or 
```
nano /home/spheron/price.json
```

## Install Provider
Set up the following variables by updating the region name and the domain.

```
REGION=[us-central-devnet]
DOMAIN=[provider.devnetdsphn.com]
```

Install the helm charts:

```
sudo su spheron
cd /home/spheron/provider-helm-charts/charts
helm upgrade --install spheron-provider ./spheron-provider -n spheron-services \
        --set from=/spheron-key/wallet.json \
        --set keysecret=testPassword \
        --set domain=$DOMAIN \
        --set bidpricestrategy=shellScript \
        --set bidpricescript="$(cat /home/spheron/bidscript.sh | openssl base64 -A)" \
        --set ipoperator=false \
        --set node=spheron \
        --set log_restart_patterns="rpc node is not catching up|bid failed" \
        --set resources.limits.cpu="2" \
        --set resources.limits.memory="2Gi" \
        --set resources.requests.cpu="1" \
        --set resources.requests.memory="1Gi"
        
kubectl patch configmap spheron-provider-scripts \
      --namespace spheron-services \
      --type json \
      --patch='[{"op": "add", "path": "/data/liveness_checks.sh", "value":"#!/bin/bash\necho \"Liveness check bypassed\""}]'

kubectl rollout restart statefulset/spheron-provider -n spheron-services        
```

## Install Operators

Install the hostname and inventory operators

```
helm upgrade --install spheron-hostname-operator ./spheron-hostname-operator -n spheron-services
helm upgrade --install inventory-operator ./spheron-inventory-operator -n spheron-services
```

## Install Persistent Storage (Optional)

If you want to add persistent storage, include it in the capabilities during the provider registration. Then, execute the following commands.

```
apt-get install -y lvm2
cat > rook.yml << EOF
operatorNamespace: rook-ceph

configOverride: |
  [global]
  osd_pool_default_pg_autoscale_mode = on
  osd_pool_default_size = 1
  osd_pool_default_min_size = 1

cephClusterSpec:
  resources:

  mon:
    count: 1
  mgr:
    count: 1

  storage:
    useAllNodes: true
    useAllDevices: true
    config:
      osdsPerDevice: "1"

cephBlockPools:
  - name: spheron-deployments
    spec:
      failureDomain: host
      replicated:
        size: 1
      parameters:
        min_size: "1"
        deviceFilter: "^vd[a-z]$"
    storageClass:
      enabled: true
      name: beta1
      isDefault: false
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      parameters:
        imageFormat: "2"
        imageFeatures: layering
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        csi.storage.k8s.io/fstype: ext4

  - name: spheron-deployments
    spec:
      failureDomain: host
      replicated:
        size: 1
      parameters:
        min_size: "1"
        deviceFilter: "^sd[a-z]$"
    storageClass:
      enabled: true
      name: beta2
      isDefault: false
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      parameters:
        imageFormat: "2"
        imageFeatures: layering
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        csi.storage.k8s.io/fstype: ext4

  - name: spheron-deployments
    spec:
      failureDomain: host
      replicated:
        size: 1
      parameters:
        min_size: "1"
        deviceFilter: "^nvme[0-9]$"
    storageClass:
      enabled: true
      name: beta3
      isDefault: false
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      parameters:
        imageFormat: "2"
        imageFeatures: layering
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        csi.storage.k8s.io/fstype: ext4

  - name: spheron-nodes
    spec:
      failureDomain: host
      replicated:
        size: 1
      parameters:
        min_size: "1"
    storageClass:
      enabled: true
      name: spheron-nodes
      isDefault: false
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      parameters:
        # RBD image format. Defaults to "2".
        imageFormat: "2"
        # RBD image features. Available for imageFormat: "2". CSI RBD currently supports only `layering` feature.
        imageFeatures: layering
        # The secrets contain Ceph admin credentials.
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        # Specify the filesystem type of the volume. If not specified, csi-provisioner
        # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
        # in hyperconverged settings where the volume is mounted on the same node as the osds.
        csi.storage.k8s.io/fstype: ext4

# Do not create default Ceph file systems, object stores
cephFileSystems:
cephObjectStores:

# Spawn rook-ceph-tools, useful for troubleshooting
toolbox:
  enabled: true
  resources:
EOF

helm search repo rook-release --version v1.12.4
helm upgrade --install --wait --create-namespace -n rook-ceph rook-ceph rook-release/rook-ceph --version 1.12.4
echo "Did you update nodes in rook-ceph-cluster.values1.yml?"
#SHOWS DUPLICATE ISSUE - WORKS WHEN RUN TWICE
helm upgrade --install --create-namespace -n rook-ceph rook-ceph-cluster --set operatorNamespace=rook-ceph rook-release/rook-ceph-cluster --version 1.12.4 -f rook.yml --force

sleep 30

kubectl label sc spheron-nodes spheron.network=true
kubectl label sc beta3 spheron.network=true
kubectl label sc beta2 spheron.network=true
kubectl label sc beta1 spheron.network=true

echo "Did you update this label to the same node in rook-ceph-cluster.values1.yml?"
kubectl label node $PERSISTENT_STORAGE_NODE1 spheron.network/storageclasses=${PERSISTENT_STORAGE_NODE1_CLASS} --overwrite
kubectl label node $PERSISTENT_STORAGE_NODE2 spheron.network/storageclasses=${PERSISTENT_STORAGE_NODE3_CLASS} --overwrite
kubectl label node $PERSISTENT_STORAGE_NODE3 spheron.network/storageclasses=${PERSISTENT_STORAGE_NODE3_CLASS} --overwrite

echo "If health not OK, do this"
kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') -- bash -c "ceph health mute POOL_NO_REDUNDANCY"
```

##  Check the status of your provider

Use the following curl request to check the status of the provider. Update the hostname with your own hostname to properly test the provider status.

```
curl --insecure https://[hostname]:8443/status
```

<Navigation name="Setup Provider" />
